{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEES-cFV3pm-",
    "outputId": "01928eea-1d39-416f-d06e-795bb54b7ce6"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(pickle.__version__)? (203225737.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Maeva\\AppData\\Local\\Temp\\ipykernel_24116\\203225737.py\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    print pickle.__version__\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(pickle.__version__)?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn import manifold, decomposition\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "RqvBvnOU3pp9",
    "outputId": "6223574b-f2f5-4f4d-f7bc-5e7f41f19fa0"
   },
   "outputs": [],
   "source": [
    "sample_posts = pd.read_csv(\"data/subsetTop30Tags2.csv\", sep=\";\")\n",
    "sample_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLkWfhZ17Zw1",
    "outputId": "c9f48c08-f01a-4d75-91c9-0a4621bd0c9c"
   },
   "outputs": [],
   "source": [
    "sample_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9w1Kgdrz7bW9",
    "outputId": "5d65bd33-4b7c-440a-bd09-f25ec7234d85"
   },
   "outputs": [],
   "source": [
    "sample_posts = sample_posts.sample(5000)\n",
    "sample_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhS9pLOinb6n"
   },
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgxvWDox7sx2",
    "outputId": "0dbe7eb6-057c-475c-9bbb-45ce67583b59"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24116\\1119194669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32myield\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# deacc=True removes punctuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_posts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFullPosts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_posts' is not defined"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(sample_posts.FullPosts))\n",
    "\n",
    "print(data_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1roj3F47w62",
    "outputId": "faafe11c-4713-4b81-9fac-1c1634001996"
   },
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV6PeqwG-VV3"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=2,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             max_features=50000             # max number of uniq words\n",
    "                            )\n",
    "# vectorizer.fit(data_lemmatized)\n",
    "# data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(data_vectorized, open('../Preprocessing/vectorizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "# print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ctub2lh-WHF",
    "outputId": "d9a780fd-f328-4170-ed87-7c82738aca2e"
   },
   "outputs": [],
   "source": [
    "# # Materialize the sparse data\n",
    "# data_dense = data_vectorized.todense()\n",
    "\n",
    "# # Compute Sparsicity = Percentage of Non-Zero cells\n",
    "# print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j6g6NLt-VYh"
   },
   "outputs": [],
   "source": [
    "X = sample_posts[\"FullPosts\"]\n",
    "y = sample_posts[\"Tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('../Preprocessing/vectorizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel binarizer for targets\n",
    "# multilabel_binarizer = MultiLabelBinarizer()\n",
    "# multilabel_binarizer.fit(y)\n",
    "# y_binarized = multilabel_binarizer.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGKMa-s3-Vd2"
   },
   "outputs": [],
   "source": [
    "# Split from the loaded dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8azZh8pJ-VgQ",
    "outputId": "2a54cabc-9dfa-4691-ce69-0c9d64f2fe78"
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape : {}\".format(X_train.shape))\n",
    "print(\"X_test shape : {}\".format(X_test.shape))\n",
    "print(\"y_train shape : {}\".format(y_train.shape))\n",
    "print(\"y_test shape : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozowjI5oAmUO"
   },
   "outputs": [],
   "source": [
    "def lsa_reduction(X_train, X_test, n_comp=120):\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    normalizer = Normalizer()\n",
    "    \n",
    "    lsa_pipe = Pipeline([('svd', svd),\n",
    "                        ('normalize', normalizer)]).fit(X_train)\n",
    "    \n",
    "    train_reduced = lsa_pipe.transform(X_train)\n",
    "    test_reduced = lsa_pipe.transform(X_test)\n",
    "    return train_reduced, test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une instance de CountVectorizer pour la vectorisation de texte\n",
    "# vect = CountVectorizer()\n",
    "\n",
    "# Créer une instance de SVC pour l'apprentissage multi-étiquettes\n",
    "svc = SVC(kernel='linear')\n",
    "lsvc = LinearSVC(random_state=0)\n",
    "\n",
    "# Créer une instance de OneVsRestClassifier pour effectuer la classification multi-étiquettes\n",
    "model_OVR = OneVsRestClassifier(svc)\n",
    "\n",
    "# Créer un pipeline qui combine la vectorisation de texte et la classification multi-étiquettes\n",
    "# model_OVR = Pipeline([('vect', vect), ('ovr', ovr)])\n",
    "\n",
    "# Entraîner le pipeline sur les données d'entraînement\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "model_OVR.fit(X_train, y_train)\n",
    "\n",
    "# Évaluer le pipeline sur les données de test\n",
    "# predict on test data\n",
    "# X_test = vectorizer.transform(X_test)\n",
    "y_pred = model_OVR.predict(X_test)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_OVR, open('../ModelsAPI/model_OVR.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"Django supports Python. If you're under Linux and want to check the Python version you're using, run python -V from the command line If you want to check the Django version, open a Python console and type\"]\n",
    "# test_words = list(sent_to_words(test))\n",
    "# test_words = lemmatization(test_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "# test_words = count_vect.transform(test_words)\n",
    "pred = model_OVR.predict(vectorizer.transform(test))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([('vect', vectorizer, 'clf', model_OVR)])\n",
    "\n",
    "# # train classifier\n",
    "# pipeline.fit(X_train)\n",
    "\n",
    "# evaluate all steps on test set\n",
    "# predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "SOMMAIRE",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
